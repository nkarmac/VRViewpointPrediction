<!DOCTYPE HTML>
<html>
	<head>
		<title>Predictive FOV Generation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
					</header>

				<!-- Nav -->
				<nav id="nav">
					<ul class="links">
						<li><a href="index.html">Latest</a></li>
						<li><a href="about.html">About</a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<h1>Predictive FOV<br />
									Generation</h1>
									<p></p>
								</header>
								<div class="image main"><img src="images/fovgen.PNG" alt="" /></div>

								<p> 
									&emsp; One of the first approaches to reduce computation and bandwidth for 360 video was to only render and display
									the current Field of View (FOV) that the user is currently viewing. Generally, 360 video is rendered as a 
									<span class="image right"><img src="images/360videoframe.jpeg" alt="" /></span>
									2D "fisheye" video projected onto a sphere, meaning that the entire video space around the user is rendered
									or streamed. The right picture shows an example video frame that would be projected onto a sphere around the user.
									Clearly, only rendering the small portion of the frame that the user is currently viewing will greatly reduce
									computation and bandwidth. 
								</p>

								<p>
									&emsp; Unfortunately, the extra time required to determine the user's viewpoint in real-time
									adds latency. Latency is potentially the largest complication for 360 video. VR is interactive video, it is meant to emulate
									real life as close as possible. Adding a delay to video rotations will detach the video from what the user expects, 
									thus likely inducing motion sickness. Xueshi Hou et al propose a deep-learning based viewpoint prediction model
									to determine the next FOV frame to be generated. By predicting the next FOV in advance, latency is effectively removed.
								</p>

								<h3>Viewpoint Prediction Model</h3>

								<p>
									&emsp; Xueshi Hou et al attained data for over 36 000 viewers of various interactive 360 videos.
									The diversity of videos and amount of viewers should allow for consistent result.
									The data includes head position, rotation, and velocity temporal data taken every 200ms over each video's duration.
									By separating the head rotational motion vector and speed from the velocity, certain periods of data can be 
									categorized as a <i>medium motion sequence</i> ("regular" movement) or a <i>high motion sequence</i> 
									(high speed unpredictable movement). The sequences are fed separately into the deep-learning model.
								</p>

								<p>
									<span class="image right"><img src="images/fovgrid.PNG" alt="" /></span>
									&emsp; The 360-degree view can be divided into 72 tiles. The rotational temporal data is converted into
									a one-hot 72x10 matrix representing possible viewpoints with a lookahead of 2 seconds (72 possible tiles within
									view and 200ms*10 = 2 seconds of lookahead). The data is trained on a Long Short-Term Memory (LSTM)
									neural network model. The model will attempt to predict one or multiple tiles that the viewpoint will be in for the next point.
									Xueshi Hou et al trained 90% of the data and tested 10%.
								</p>

								<h3>FOV Generation</h3>

								<p>
									&emsp; A predicted FOV can be generated by selecting the group of tiles with the highest concentration of 1's within the 72x10
									viewpoint prediction matrix. The area of the video projected sphere corresponding to this FOV is then rendered and transmitted 
									in high quality, and the rest of the video sphere is either not rendered, or transmitted in low quality. By increasing the number 
									of tiles in a group, the success rate increases, but this counteracts the initial objective by also increasing computation and 
									bandwidth.
								</p>
								
								<p>
									&emsp; Xueshi Hou et al were able to attain a consistent FOV viewpoint prediction accuracy of 95%. Even when reducing the number of tiles
									in the generated FOV to as low as 4, the accuracy still maintained a consistent 95%. This results in a bandwidth reduction (pixel saving)
									of up to 71%. This demonstrates that the LSTM model for viewpoint prediction can significantly lower computation and bandwidth
									while maintaining the perceived video quality by the user.
								</p>
								

								<b>Further Reading: <a href="https://dl.acm.org/doi/abs/10.1145/3229625.3229629">Predictive View Generation to Enable Mobile 360-degree and VR Experiences</a></b>
							</section>

					</div>

				<!-- Copyright -->
				<div id="copyright">
					<ul><li><a href="https://html5up.net">HTML5 UP</a></li></ul>
				</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>